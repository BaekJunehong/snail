{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Njl9YkGxN7WX"
      },
      "source": [
        "# Step 0. 환경 구성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GGP5cCroKo2"
      },
      "source": [
        "구글 드라이브와 Colab을 연동"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfJsHw8VoKDB",
        "outputId": "9ab0f84c-5470-4b7a-e1fc-58160f65aa41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnX1gx5vv_kc"
      },
      "source": [
        "\n",
        "Gpu 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KStnU69Dv-zz",
        "outputId": "433d6a20-d5ed-4aa7-b39d-f28e846ba287"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnDQzwwLwC-c"
      },
      "source": [
        "whisper ai 사용에 필요한 python 패키지"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86z0bIqxeMki",
        "outputId": "820abe6b-e999-4371-b869-f3f67e99dea0"
      },
      "outputs": [],
      "source": [
        "# 데이터셋과 transformers 설치\n",
        "!pip install datasets>=2.6.1\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "\n",
        "# 오디오 처리를 위한 librosa 설치\n",
        "!pip install librosa\n",
        "\n",
        "# 성능 측정을 위한 evaluate와 jiwer 설치\n",
        "!pip install evaluate>=0.30\n",
        "!pip install jiwer\n",
        "\n",
        "# 인터랙티브 인터페이스를 위한 gradio 설치\n",
        "!pip install gradio\n",
        "\n",
        "# Transformers와 PyTorch를 함께 사용하기 위한 accelerate 설치\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate>=0.20.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV65OMc_wGrJ"
      },
      "source": [
        "modules 과 packges import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JcHXA33leT0A"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTo-ygLdwTwp"
      },
      "source": [
        "# Step 1. 함수 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk-vZCszr3ZA"
      },
      "source": [
        "데이터 콜레이터 클래스는 ASR 모델의 훈련 및 평가 과정에서 데이터의 전처리와 패딩을 처리하는 역할을 수행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQEmML4Zex5z"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    \"\"\"\n",
        "    Use Data Collator to perform Speech Seq2Seq with padding\n",
        "    \"\"\"\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcpIB7Pnr5aR"
      },
      "source": [
        "평기지표: CER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg9QFaBGe2nj",
        "outputId": "80d58998-742f-4b5c-9491-2d5c1eaebb65"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-c81d87c6f9c2>:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  cer_metric = load_metric(\"cer\")\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "cer_metric = load_metric(\"cer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riZcRM4or7RH"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
        "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"cer\": cer}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYXaMpOtw6Lm"
      },
      "source": [
        "# Step 2. 음성 처리에 필요한 기능"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ljILt1DpwcJ"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n",
        "\n",
        "# Load Feature extractor: WhisperFeatureExtractor\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-base\")\n",
        "\n",
        "# Load Tokenizer: WhisperTokenizer\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base\", language=\"korean\", task=\"transcribe\")\n",
        "\n",
        "# Load Processor: WhisperProcessor\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\", language=\"korean\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBajQzD5QgEL"
      },
      "source": [
        "\n",
        "*   WhisperFeatureExtractor는 사전 훈련된 \"openai/whisper-base\" 모델을 로드\n",
        "음성 데이터를 특성으로 변환\n",
        "\n",
        "*   WhisperTokenizer도 \"openai/whisper-base\" 모델을 로드\n",
        "한국어 음성 처리를 위해 설정되며, \"transcribe\" 작업을 수행할 수 있도록 토큰화 기능을 제공\n",
        "\n",
        "*   WhisperProcessor는 \"openai/whisper-base\" 모델을 로드\n",
        "한국어 음성 처리와 \"transcribe\" 작업을 수행하는데 사용\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSLj6zSXKxbT"
      },
      "source": [
        "# Step 3. 데이터셋 불러오기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgb0WJGCRl5T"
      },
      "source": [
        "이전에 map함수를 이용해 전처리를 진행하고 저장했던 데이터 샛을 불러오는 과정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSIO3cKHKw97"
      },
      "outputs": [],
      "source": [
        "from datasets import load_from_disk\n",
        "\n",
        "# Load the combined 'train' dataset\n",
        "loaded_combined_train_dataset = load_from_disk('/content/drive/MyDrive/data_file/combined_50000_train_dataset')\n",
        "\n",
        "# Load the combined 'test' dataset\n",
        "loaded_combined_test_dataset = load_from_disk('/content/drive/MyDrive/data_file/combined_50000_test_dataset')\n",
        "\n",
        "# Now you can use loaded_combined_train_dataset and loaded_combined_test_dataset as needed\n",
        "# For example, you can access specific examples using indexing like loaded_combined_train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2J25ZVYTVnd"
      },
      "source": [
        "train 데이터와 test 데이터 갯수 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnxRCmD3TSWx",
        "outputId": "a4db47ea-8e85-488b-cb06-51072ca205ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['input_features', 'labels'],\n",
            "    num_rows: 50000\n",
            "})\n",
            "Dataset({\n",
            "    features: ['input_features', 'labels'],\n",
            "    num_rows: 2000\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(loaded_combined_train_dataset)\n",
        "print(loaded_combined_test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRFES7w9xava"
      },
      "source": [
        "# Step 4. 학습 및 평가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpJaQkBYpm_w"
      },
      "outputs": [],
      "source": [
        "# STEP 5.1. Initialize the Data collator\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
        "\n",
        "# STEP 5.1. Define evaluation metric\n",
        "import evaluate\n",
        "metric = evaluate.load(\"cer\")\n",
        "\n",
        "# STEP 5.3. Load a pre-trained Checkpoint\n",
        "from transformers import WhisperForConditionalGeneration\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
        "\n",
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "9gGTa9HVqXov",
        "outputId": "9c8656db-f292-4373-fbce-8b81200c3888"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training 시작\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4000/4000 13:05:48, Epoch 5/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.857700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.379500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.320500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.271500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.261600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.212100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.203300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.191500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.161400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.159900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>0.143700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>0.136200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>0.135500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training 완료\n"
          ]
        }
      ],
      "source": [
        "# STEP 5.4. Define the training configuration\n",
        "\"\"\"\n",
        "Check for Seq2SeqTrainingArguments here:\n",
        "https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments\n",
        "\"\"\"\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper_base_0824_ver1\",  # 저장된 모델 및 결과물의 디렉토리 경로\n",
        "    per_device_train_batch_size=32,  # 한 번에 처리되는 훈련 배치 크기\n",
        "    gradient_accumulation_steps=2,  # 배치 크기 감소시 그래디언트 누적을 통한 학습 안정화\n",
        "    learning_rate=1e-5,  # 학습률\n",
        "    warmup_steps=400,  # 초기 학습률 조정을 위한 웜업 스텝 수 / 일반적으로는 10% ~ 20%의 전체 학습 스텝 수에 해당하는 값을 시도\n",
        "    max_steps=4000,  # 전체 훈련 스텝 수\n",
        "    gradient_checkpointing=True,  # 그래디언트 체크포인팅을 통한 메모리 절약\n",
        "    fp16=True,  # FP16 형식으로 훈련 수행 (반정밀도 부동소수점)( cpu 가동시 안씀)\n",
        "    evaluation_strategy=\"no\",  # 검증 수행 전략 설정\n",
        "    per_device_eval_batch_size=16,  # 한 번에 처리되는 검증 배치 크기\n",
        "    predict_with_generate=True,  # 생성된 토큰을 통해 예측 수행\n",
        "    generation_max_length=225,  # 생성된 토큰의 최대 길이 (225유지)\n",
        "    eval_steps=300,  # 검증 수행 스텝 수\n",
        "    logging_steps=300,  # 로그 기록 스텝 수\n",
        "    load_best_model_at_end=False,  # 훈련 종료 시 최적 모델 로드 여부\n",
        "    metric_for_best_model=\"cer\",  # 최적 모델 선정을 위한 평가 지표 wer -> cer로 변경\n",
        "    greater_is_better=False,  # 평가 지표 값이 높을수록 좋은지 여부\n",
        "    save_steps=300  # 변경된 save_steps 값\n",
        ")\n",
        "\n",
        "# Initialize a trainer.\n",
        "\"\"\"\n",
        "Forward the training arguments to the Hugging Face trainer along with our model,\n",
        "dataset, data collator and compute_metrics function.\n",
        "\"\"\"\n",
        "# 지정된 인자 및 구성요소로 트레이너를 초기화합니다\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,                   # 이전에 정의한 훈련 인자\n",
        "    model=model,                          # 훈련할 ASR 모델\n",
        "    train_dataset=loaded_combined_train_dataset,# 훈련 데이터셋\n",
        "    eval_dataset=loaded_combined_test_dataset,  # 평가 데이터셋\n",
        "    data_collator=data_collator,           # 데이터 전처리를 위한 데이터 콜레이터\n",
        "    compute_metrics=compute_metrics,          # CER 메트릭을 계산하는 함수\n",
        "    tokenizer=processor.feature_extractor, # 입력 오디오 데이터를 처리하기 위한 토크나이저\n",
        ")\n",
        "\n",
        "# Save processor object before starting training\n",
        "processor.save_pretrained(training_args.output_dir)\n",
        "\n",
        "# STEP 5.5. Training\n",
        "\"\"\"\n",
        "Training will take appr. 5-10 hours depending on your GPU.\n",
        "\"\"\"\n",
        "print('Training 시작')\n",
        "trainer.train()   # <-- training 시작\n",
        "print('Training 완료')\n",
        "\n",
        "#\"Step\": 모델의 훈련 과정에서 진행되는 각 스텝을 나타내는 숫자입니다.\n",
        "#스텝은 주로 배치(batch) 단위로 모델이 업데이트되는 지점을 의미합니다.\n",
        "\n",
        "# Training Loss는 모델이 훈련 데이터에 대해 얼마나 정확하게 예측하는지를 나타내는 지표\n",
        "# Training Loss가 감소하면 모델이 훈련 데이터에 대해 더 잘 학습하고 있는 것\n",
        "# 모델이 데이터에 더 잘 적합되고 있다는 것을 의미\n",
        "\n",
        "# Validation Loss는 모델이 이전에 본 적이 없는 검증 데이터에 대한 예측 정확도\n",
        "# 훈련 과정 중에 일정 주기마다 검증 데이터를 사용하여 Validation Loss를 계산\n",
        "# 이 값이 감소하면 모델이 일반화되고 있는 것을 의미\n",
        "# 모델이 훈련 데이터뿐만 아니라 새로운 데이터에도 잘 예측할 수 있도록 학습되고 있다는 것\n",
        "\n",
        "# \"CER\" (Character Error Rate): 훈련 중에 일정 주기마다 검증 데이터를 사용하여\n",
        "#모델의 문자 에러 비율(CER)을 평가한 값입니다.\n",
        "#CER은 텍스트 분야에서 자주 사용되는 평가 지표 중 하나로,\n",
        "#모델이 생성한 텍스트와 실제 텍스트 사이의 문자 수준 오류 비율을 나타냅니다.\n",
        "#CER이 낮을수록 모델의 성능이 좋다고 판단됩니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfjeBCZN5bt4"
      },
      "source": [
        "학습된 모델 평가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "Mt2IEIYirgRF",
        "outputId": "31feec2c-c61f-45b2-c4e5-1460012ed739"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 06:41]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.28062090277671814,\n",
              " 'eval_cer': 0.11021811039900765,\n",
              " 'eval_runtime': 548.047,\n",
              " 'eval_samples_per_second': 3.649,\n",
              " 'eval_steps_per_second': 0.228,\n",
              " 'epoch': 5.12}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGGPr1sGPDSD"
      },
      "source": [
        "# Step 5. 모델 저장"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZM_ZjeiKntP"
      },
      "source": [
        "학습된 모델 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VYMWMJ9wbAy"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/model/whisper_base_0824\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gjk-9EI6KlYi"
      },
      "source": [
        "모델에 토크나이저 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpqJq9FGrYtz",
        "outputId": "bebf6d00-0477-4eba-8efb-c6b2a60125c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/model/whisper_base_0824/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/model/whisper_base_0824/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/model/whisper_base_0824/vocab.json',\n",
              " '/content/drive/MyDrive/model/whisper_base_0824/merges.txt',\n",
              " '/content/drive/MyDrive/model/whisper_base_0824/normalizer.json',\n",
              " '/content/drive/MyDrive/model/whisper_base_0824/added_tokens.json',\n",
              " '/content/drive/MyDrive/model/whisper_base_0824/tokenizer.json')"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the trained tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/whisper_base_0824_ver1\")\n",
        "\n",
        "# Specify the directory where you want to save the tokenizer\n",
        "save_directory = \"/content/drive/MyDrive/model/whisper_base_0824\"\n",
        "\n",
        "# Save the tokenizer to the specified directory\n",
        "tokenizer.save_pretrained(save_directory)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lnx1jP_PKrnZ"
      },
      "source": [
        "모델 파일내용 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjiPQixprXJk",
        "outputId": "49240ec3-823a-454c-8ac6-8709175d8e82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['config.json', 'generation_config.json', 'pytorch_model.bin', 'preprocessor_config.json', 'training_args.bin', 'tokenizer_config.json', 'special_tokens_map.json', 'added_tokens.json', 'vocab.json', 'merges.txt', 'normalizer.json', 'tokenizer.json']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "model_path = \"/content/drive/MyDrive/model/whisper_base_0824\"\n",
        "files_in_model_path = os.listdir(model_path)\n",
        "print(files_in_model_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eEWkZxnPPdh"
      },
      "source": [
        "# Step 6. 모델 실행"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjz-ZITcKubs"
      },
      "source": [
        "모델 실행코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-_tVW79rU2y",
        "outputId": "21a725e9-32d8-4131-ce8b-169a106314b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "음성 파일: /content/drive/MyDrive/data_file/평가 데이터셋/K00013886-BFG23-L1N2D2-E-K0KK-02601769.wav\n",
            "텍스트 출력:  시험 일주일 남았는데 게임을 안한다\n",
            "\n",
            "음성 파일: /content/drive/MyDrive/data_file/평가 데이터셋/K00013886-BFG23-L1N2D2-E-K0KK-02743434.wav\n",
            "텍스트 출력: 다음엔 나도 데리고 가라 해야지\n",
            "\n",
            "음성 파일: /content/drive/MyDrive/data_file/평가 데이터셋/K00013886-BFG23-L1N2D2-E-K0KK-02989139.wav\n",
            "텍스트 출력:  모자를 쓰니 좀 시원한 느낌이다.\n",
            "\n",
            "음성 파일: /content/drive/MyDrive/data_file/평가 데이터셋/K00014982-BFG20-L1N2D1-E-K0KK-03006747.wav\n",
            "텍스트 출력:  한자공부를 하니 모르는 단어를 많이 알게 되었어요.\n",
            "\n",
            "음성 파일: /content/drive/MyDrive/data_file/평가 데이터셋/K00014982-BFG20-L1N2D1-E-K0KK-03017978.wav\n",
            "텍스트 출력: 자원을 재활용하는 건 지구를 위해서 좋은 거래요\n",
            "\n",
            "음성 파일: /content/drive/MyDrive/data_file/평가 데이터셋/K00014982-BFG20-L1N2D4-E-K0KK-02872759.wav\n",
            "텍스트 출력:  책상 위에 쓰레기가 많아요.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import soundfile as sf  # soundfile 라이브러리 사용\n",
        "\n",
        "# ASR 파이프라인 초기화\n",
        "model_name_or_path = \"/content/drive/MyDrive/model/whisper_base_0824\"\n",
        "asr = pipeline(model=model_name_or_path, task=\"automatic-speech-recognition\")\n",
        "\n",
        "# 음성 파일 경로 리스트\n",
        "audio_file_paths = [\n",
        "    \"/content/drive/MyDrive/data_file/평가 데이터셋/K00013886-BFG23-L1N2D2-E-K0KK-02601769.wav\",\n",
        "    \"/content/drive/MyDrive/data_file/평가 데이터셋/K00013886-BFG23-L1N2D2-E-K0KK-02743434.wav\",\n",
        "    \"/content/drive/MyDrive/data_file/평가 데이터셋/K00013886-BFG23-L1N2D2-E-K0KK-02989139.wav\",\n",
        "    \"/content/drive/MyDrive/data_file/평가 데이터셋/K00014982-BFG20-L1N2D1-E-K0KK-03006747.wav\",\n",
        "    \"/content/drive/MyDrive/data_file/평가 데이터셋/K00014982-BFG20-L1N2D1-E-K0KK-03017978.wav\",\n",
        "    \"/content/drive/MyDrive/data_file/평가 데이터셋/K00014982-BFG20-L1N2D4-E-K0KK-02872759.wav\",\n",
        "    # 추가 음성 파일 경로\n",
        "]\n",
        "\n",
        "# ASR 함수 정의\n",
        "def transcribe_audio(audio_path):\n",
        "    transcription = asr(audio_path)\n",
        "    return transcription['text']  # Use 'text' key to get the transcribed text\n",
        "\n",
        "# 각 음성 파일에 대한 처리 및 출력\n",
        "for audio_file_path in audio_file_paths:\n",
        "    transcription_text = transcribe_audio(audio_file_path)\n",
        "    print(f\"음성 파일: {audio_file_path}\")\n",
        "    print(\"텍스트 출력:\", transcription_text)\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Njl9YkGxN7WX",
        "WTo-ygLdwTwp",
        "fYXaMpOtw6Lm",
        "SSLj6zSXKxbT"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
