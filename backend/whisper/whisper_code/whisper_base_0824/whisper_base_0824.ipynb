{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Njl9YkGxN7WX"
      },
      "source": [
        "# Step 0. í™˜ê²½ êµ¬ì„±"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GGP5cCroKo2"
      },
      "source": [
        "êµ¬ê¸€ ë“œë¼ì´ë¸Œì™€ Colabì„ ì—°ë™"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfJsHw8VoKDB",
        "outputId": "9ab0f84c-5470-4b7a-e1fc-58160f65aa41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnX1gx5vv_kc"
      },
      "source": [
        "\n",
        "Gpu í™•ì¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KStnU69Dv-zz",
        "outputId": "433d6a20-d5ed-4aa7-b39d-f28e846ba287"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnDQzwwLwC-c"
      },
      "source": [
        "whisper ai ì‚¬ìš©ì— í•„ìš”í•œ python íŒ¨í‚¤ì§€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86z0bIqxeMki",
        "outputId": "820abe6b-e999-4371-b869-f3f67e99dea0"
      },
      "outputs": [],
      "source": [
        "# ë°ì´í„°ì…‹ê³¼ transformers ì„¤ì¹˜\n",
        "!pip install datasets>=2.6.1\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "\n",
        "# ì˜¤ë””ì˜¤ ì²˜ë¦¬ë¥¼ ìœ„í•œ librosa ì„¤ì¹˜\n",
        "!pip install librosa\n",
        "\n",
        "# ì„±ëŠ¥ ì¸¡ì •ì„ ìœ„í•œ evaluateì™€ jiwer ì„¤ì¹˜\n",
        "!pip install evaluate>=0.30\n",
        "!pip install jiwer\n",
        "\n",
        "# ì¸í„°ë™í‹°ë¸Œ ì¸í„°í˜ì´ìŠ¤ë¥¼ ìœ„í•œ gradio ì„¤ì¹˜\n",
        "!pip install gradio\n",
        "\n",
        "# Transformersì™€ PyTorchë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ê¸° ìœ„í•œ accelerate ì„¤ì¹˜\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate>=0.20.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV65OMc_wGrJ"
      },
      "source": [
        "modules ê³¼ packges import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JcHXA33leT0A"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTo-ygLdwTwp"
      },
      "source": [
        "# Step 1. í•¨ìˆ˜ ì •ì˜"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk-vZCszr3ZA"
      },
      "source": [
        "ë°ì´í„° ì½œë ˆì´í„° í´ë˜ìŠ¤ëŠ” ASR ëª¨ë¸ì˜ í›ˆë ¨ ë° í‰ê°€ ê³¼ì •ì—ì„œ ë°ì´í„°ì˜ ì „ì²˜ë¦¬ì™€ íŒ¨ë”©ì„ ì²˜ë¦¬í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQEmML4Zex5z"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    \"\"\"\n",
        "    Use Data Collator to perform Speech Seq2Seq with padding\n",
        "    \"\"\"\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcpIB7Pnr5aR"
      },
      "source": [
        "í‰ê¸°ì§€í‘œ: CER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg9QFaBGe2nj",
        "outputId": "80d58998-742f-4b5c-9491-2d5c1eaebb65"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-c81d87c6f9c2>:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  cer_metric = load_metric(\"cer\")\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "cer_metric = load_metric(\"cer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riZcRM4or7RH"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
        "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"cer\": cer}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYXaMpOtw6Lm"
      },
      "source": [
        "# Step 2. ìŒì„± ì²˜ë¦¬ì— í•„ìš”í•œ ê¸°ëŠ¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ljILt1DpwcJ"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n",
        "\n",
        "# Load Feature extractor: WhisperFeatureExtractor\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-base\")\n",
        "\n",
        "# Load Tokenizer: WhisperTokenizer\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base\", language=\"korean\", task=\"transcribe\")\n",
        "\n",
        "# Load Processor: WhisperProcessor\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\", language=\"korean\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBajQzD5QgEL"
      },
      "source": [
        "\n",
        "*   WhisperFeatureExtractorëŠ” ì‚¬ì „ í›ˆë ¨ëœ \"openai/whisper-base\" ëª¨ë¸ì„ ë¡œë“œ\n",
        "ìŒì„± ë°ì´í„°ë¥¼ íŠ¹ì„±ìœ¼ë¡œ ë³€í™˜\n",
        "\n",
        "*   WhisperTokenizerë„ \"openai/whisper-base\" ëª¨ë¸ì„ ë¡œë“œ\n",
        "í•œêµ­ì–´ ìŒì„± ì²˜ë¦¬ë¥¼ ìœ„í•´ ì„¤ì •ë˜ë©°, \"transcribe\" ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í† í°í™” ê¸°ëŠ¥ì„ ì œê³µ\n",
        "\n",
        "*   WhisperProcessorëŠ” \"openai/whisper-base\" ëª¨ë¸ì„ ë¡œë“œ\n",
        "í•œêµ­ì–´ ìŒì„± ì²˜ë¦¬ì™€ \"transcribe\" ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ”ë° ì‚¬ìš©\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSLj6zSXKxbT"
      },
      "source": [
        "# Step 3. ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgb0WJGCRl5T"
      },
      "source": [
        "ì´ì „ì— mapí•¨ìˆ˜ë¥¼ ì´ìš©í•´ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•˜ê³  ì €ì¥í–ˆë˜ ë°ì´í„° ìƒ›ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ê³¼ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSIO3cKHKw97"
      },
      "outputs": [],
      "source": [
        "from datasets import load_from_disk\n",
        "\n",
        "# Load the combined 'train' dataset\n",
        "loaded_combined_train_dataset = load_from_disk('/content/drive/MyDrive/data_file/combined_50000_train_dataset')\n",
        "\n",
        "# Load the combined 'test' dataset\n",
        "loaded_combined_test_dataset = load_from_disk('/content/drive/MyDrive/data_file/combined_50000_test_dataset')\n",
        "\n",
        "# Now you can use loaded_combined_train_dataset and loaded_combined_test_dataset as needed\n",
        "# For example, you can access specific examples using indexing like loaded_combined_train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2J25ZVYTVnd"
      },
      "source": [
        "train ë°ì´í„°ì™€ test ë°ì´í„° ê°¯ìˆ˜ í™•ì¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnxRCmD3TSWx",
        "outputId": "a4db47ea-8e85-488b-cb06-51072ca205ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['input_features', 'labels'],\n",
            "    num_rows: 50000\n",
            "})\n",
            "Dataset({\n",
            "    features: ['input_features', 'labels'],\n",
            "    num_rows: 2000\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(loaded_combined_train_dataset)\n",
        "print(loaded_combined_test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRFES7w9xava"
      },
      "source": [
        "# Step 4. í•™ìŠµ ë° í‰ê°€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpJaQkBYpm_w"
      },
      "outputs": [],
      "source": [
        "# STEP 5.1. Initialize the Data collator\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
        "\n",
        "# STEP 5.1. Define evaluation metric\n",
        "import evaluate\n",
        "metric = evaluate.load(\"cer\")\n",
        "\n",
        "# STEP 5.3. Load a pre-trained Checkpoint\n",
        "from transformers import WhisperForConditionalGeneration\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
        "\n",
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "9gGTa9HVqXov",
        "outputId": "9c8656db-f292-4373-fbce-8b81200c3888"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training ì‹œì‘\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4000/4000 13:05:48, Epoch 5/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.857700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.379500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.320500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.271500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.261600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.212100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.203300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.191500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.161400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.159900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>0.143700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>0.136200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>0.135500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training ì™„ë£Œ\n"
          ]
        }
      ],
      "source": [
        "# STEP 5.4. Define the training configuration\n",
        "\"\"\"\n",
        "Check for Seq2SeqTrainingArguments here:\n",
        "https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments\n",
        "\"\"\"\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper_base_0824_ver1\",  # ì €ì¥ëœ ëª¨ë¸ ë° ê²°ê³¼ë¬¼ì˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ\n",
        "    per_device_train_batch_size=32,  # í•œ ë²ˆì— ì²˜ë¦¬ë˜ëŠ” í›ˆë ¨ ë°°ì¹˜ í¬ê¸°\n",
        "    gradient_accumulation_steps=2,  # ë°°ì¹˜ í¬ê¸° ê°ì†Œì‹œ ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì ì„ í†µí•œ í•™ìŠµ ì•ˆì •í™”\n",
        "    learning_rate=1e-5,  # í•™ìŠµë¥ \n",
        "    warmup_steps=400,  # ì´ˆê¸° í•™ìŠµë¥  ì¡°ì •ì„ ìœ„í•œ ì›œì—… ìŠ¤í… ìˆ˜ / ì¼ë°˜ì ìœ¼ë¡œëŠ” 10% ~ 20%ì˜ ì „ì²´ í•™ìŠµ ìŠ¤í… ìˆ˜ì— í•´ë‹¹í•˜ëŠ” ê°’ì„ ì‹œë„\n",
        "    max_steps=4000,  # ì „ì²´ í›ˆë ¨ ìŠ¤í… ìˆ˜\n",
        "    gradient_checkpointing=True,  # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…ì„ í†µí•œ ë©”ëª¨ë¦¬ ì ˆì•½\n",
        "    fp16=True,  # FP16 í˜•ì‹ìœ¼ë¡œ í›ˆë ¨ ìˆ˜í–‰ (ë°˜ì •ë°€ë„ ë¶€ë™ì†Œìˆ˜ì )( cpu ê°€ë™ì‹œ ì•ˆì”€)\n",
        "    evaluation_strategy=\"no\",  # ê²€ì¦ ìˆ˜í–‰ ì „ëµ ì„¤ì •\n",
        "    per_device_eval_batch_size=16,  # í•œ ë²ˆì— ì²˜ë¦¬ë˜ëŠ” ê²€ì¦ ë°°ì¹˜ í¬ê¸°\n",
        "    predict_with_generate=True,  # ìƒì„±ëœ í† í°ì„ í†µí•´ ì˜ˆì¸¡ ìˆ˜í–‰\n",
        "    generation_max_length=225,  # ìƒì„±ëœ í† í°ì˜ ìµœëŒ€ ê¸¸ì´ (225ìœ ì§€)\n",
        "    eval_steps=300,  # ê²€ì¦ ìˆ˜í–‰ ìŠ¤í… ìˆ˜\n",
        "    logging_steps=300,  # ë¡œê·¸ ê¸°ë¡ ìŠ¤í… ìˆ˜\n",
        "    load_best_model_at_end=False,  # í›ˆë ¨ ì¢…ë£Œ ì‹œ ìµœì  ëª¨ë¸ ë¡œë“œ ì—¬ë¶€\n",
        "    metric_for_best_model=\"cer\",  # ìµœì  ëª¨ë¸ ì„ ì •ì„ ìœ„í•œ í‰ê°€ ì§€í‘œ wer -> cerë¡œ ë³€ê²½\n",
        "    greater_is_better=False,  # í‰ê°€ ì§€í‘œ ê°’ì´ ë†’ì„ìˆ˜ë¡ ì¢‹ì€ì§€ ì—¬ë¶€\n",
        "    save_steps=300  # ë³€ê²½ëœ save_steps ê°’\n",
        ")\n",
        "\n",
        "# Initialize a trainer.\n",
        "\"\"\"\n",
        "Forward the training arguments to the Hugging Face trainer along with our model,\n",
        "dataset, data collator and compute_metrics function.\n",
        "\"\"\"\n",
        "# ì§€ì •ëœ ì¸ì ë° êµ¬ì„±ìš”ì†Œë¡œ íŠ¸ë ˆì´ë„ˆë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,                   # ì´ì „ì— ì •ì˜í•œ í›ˆë ¨ ì¸ì\n",
        "    model=model,                          # í›ˆë ¨í•  ASR ëª¨ë¸\n",
        "    train_dataset=loaded_combined_train_dataset,# í›ˆë ¨ ë°ì´í„°ì…‹\n",
        "    eval_dataset=loaded_combined_test_dataset,  # í‰ê°€ ë°ì´í„°ì…‹\n",
        "    data_collator=data_collator,           # ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°ì´í„° ì½œë ˆì´í„°\n",
        "    compute_metrics=compute_metrics,          # CER ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜\n",
        "    tokenizer=processor.feature_extractor, # ì…ë ¥ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ í† í¬ë‚˜ì´ì €\n",
        ")\n",
        "\n",
        "# Save processor object before starting training\n",
        "processor.save_pretrained(training_args.output_dir)\n",
        "\n",
        "# STEP 5.5. Training\n",
        "\"\"\"\n",
        "Training will take appr. 5-10 hours depending on your GPU.\n",
        "\"\"\"\n",
        "print('Training ì‹œì‘')\n",
        "trainer.train()   # <-- training ì‹œì‘\n",
        "print('Training ì™„ë£Œ')\n",
        "\n",
        "#\"Step\": ëª¨ë¸ì˜ í›ˆë ¨ ê³¼ì •ì—ì„œ ì§„í–‰ë˜ëŠ” ê° ìŠ¤í…ì„ ë‚˜íƒ€ë‚´ëŠ” ìˆ«ìì…ë‹ˆë‹¤.\n",
        "#ìŠ¤í…ì€ ì£¼ë¡œ ë°°ì¹˜(batch) ë‹¨ìœ„ë¡œ ëª¨ë¸ì´ ì—…ë°ì´íŠ¸ë˜ëŠ” ì§€ì ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
        "\n",
        "# Training LossëŠ” ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œ\n",
        "# Training Lossê°€ ê°ì†Œí•˜ë©´ ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•´ ë” ì˜ í•™ìŠµí•˜ê³  ìˆëŠ” ê²ƒ\n",
        "# ëª¨ë¸ì´ ë°ì´í„°ì— ë” ì˜ ì í•©ë˜ê³  ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸\n",
        "\n",
        "# Validation LossëŠ” ëª¨ë¸ì´ ì´ì „ì— ë³¸ ì ì´ ì—†ëŠ” ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ì •í™•ë„\n",
        "# í›ˆë ¨ ê³¼ì • ì¤‘ì— ì¼ì • ì£¼ê¸°ë§ˆë‹¤ ê²€ì¦ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ Validation Lossë¥¼ ê³„ì‚°\n",
        "# ì´ ê°’ì´ ê°ì†Œí•˜ë©´ ëª¨ë¸ì´ ì¼ë°˜í™”ë˜ê³  ìˆëŠ” ê²ƒì„ ì˜ë¯¸\n",
        "# ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ë¿ë§Œ ì•„ë‹ˆë¼ ìƒˆë¡œìš´ ë°ì´í„°ì—ë„ ì˜ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ í•™ìŠµë˜ê³  ìˆë‹¤ëŠ” ê²ƒ\n",
        "\n",
        "# \"CER\" (Character Error Rate): í›ˆë ¨ ì¤‘ì— ì¼ì • ì£¼ê¸°ë§ˆë‹¤ ê²€ì¦ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬\n",
        "#ëª¨ë¸ì˜ ë¬¸ì ì—ëŸ¬ ë¹„ìœ¨(CER)ì„ í‰ê°€í•œ ê°’ì…ë‹ˆë‹¤.\n",
        "#CERì€ í…ìŠ¤íŠ¸ ë¶„ì•¼ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” í‰ê°€ ì§€í‘œ ì¤‘ í•˜ë‚˜ë¡œ,\n",
        "#ëª¨ë¸ì´ ìƒì„±í•œ í…ìŠ¤íŠ¸ì™€ ì‹¤ì œ í…ìŠ¤íŠ¸ ì‚¬ì´ì˜ ë¬¸ì ìˆ˜ì¤€ ì˜¤ë¥˜ ë¹„ìœ¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
        "#CERì´ ë‚®ì„ìˆ˜ë¡ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì¢‹ë‹¤ê³  íŒë‹¨ë©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfjeBCZN5bt4"
      },
      "source": [
        "í•™ìŠµëœ ëª¨ë¸ í‰ê°€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "Mt2IEIYirgRF",
        "outputId": "31feec2c-c61f-45b2-c4e5-1460012ed739"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 06:41]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.28062090277671814,\n",
              " 'eval_cer': 0.11021811039900765,\n",
              " 'eval_runtime': 548.047,\n",
              " 'eval_samples_per_second': 3.649,\n",
              " 'eval_steps_per_second': 0.228,\n",
              " 'epoch': 5.12}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGGPr1sGPDSD"
      },
      "source": [
        "# Step 5. ëª¨ë¸ ì €ì¥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZM_ZjeiKntP"
      },
      "source": [
        "í•™ìŠµëœ ëª¨ë¸ ì €ì¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VYMWMJ9wbAy"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/model/whisper_base_0824\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gjk-9EI6KlYi"
      },
      "source": [
        "ëª¨ë¸ì— í† í¬ë‚˜ì´ì € ì €ì¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpqJq9FGrYtz",
        "outputId": "bebf6d00-0477-4eba-8efb-c6b2a60125c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/model/whisper_base_0824/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/model/whisper_base_0824/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/model/whisper_base_0824/vocab.json',\n",
              " '/content/drive/MyDrive/model/whisper_base_0824/merges.txt',\n",
              " '/content/drive/MyDrive/model/whisper_base_0824/normalizer.json',\n",
              " '/content/drive/MyDrive/model/whisper_base_0824/added_tokens.json',\n",
              " '/content/drive/MyDrive/model/whisper_base_0824/tokenizer.json')"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the trained tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/whisper_base_0824_ver1\")\n",
        "\n",
        "# Specify the directory where you want to save the tokenizer\n",
        "save_directory = \"/content/drive/MyDrive/model/whisper_base_0824\"\n",
        "\n",
        "# Save the tokenizer to the specified directory\n",
        "tokenizer.save_pretrained(save_directory)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lnx1jP_PKrnZ"
      },
      "source": [
        "ëª¨ë¸ íŒŒì¼ë‚´ìš© í™•ì¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjiPQixprXJk",
        "outputId": "49240ec3-823a-454c-8ac6-8709175d8e82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['config.json', 'generation_config.json', 'pytorch_model.bin', 'preprocessor_config.json', 'training_args.bin', 'tokenizer_config.json', 'special_tokens_map.json', 'added_tokens.json', 'vocab.json', 'merges.txt', 'normalizer.json', 'tokenizer.json']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "model_path = \"/content/drive/MyDrive/model/whisper_base_0824\"\n",
        "files_in_model_path = os.listdir(model_path)\n",
        "print(files_in_model_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eEWkZxnPPdh"
      },
      "source": [
        "# Step 6. ëª¨ë¸ ì‹¤í–‰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjz-ZITcKubs"
      },
      "source": [
        "ëª¨ë¸ ì‹¤í–‰ì½”ë“œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-_tVW79rU2y",
        "outputId": "21a725e9-32d8-4131-ce8b-169a106314b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ìŒì„± íŒŒì¼: /content/drive/MyDrive/data_file/á„‘á…§á†¼á„€á…¡ á„ƒá…¦á„‹á…µá„á…¥á„‰á…¦á†º/K00013886-BFG23-L1N2D2-E-K0KK-02601769.wav\n",
            "í…ìŠ¤íŠ¸ ì¶œë ¥:  ì‹œí—˜ ì¼ì£¼ì¼ ë‚¨ì•˜ëŠ”ë° ê²Œì„ì„ ì•ˆí•œë‹¤\n",
            "\n",
            "ìŒì„± íŒŒì¼: /content/drive/MyDrive/data_file/á„‘á…§á†¼á„€á…¡ á„ƒá…¦á„‹á…µá„á…¥á„‰á…¦á†º/K00013886-BFG23-L1N2D2-E-K0KK-02743434.wav\n",
            "í…ìŠ¤íŠ¸ ì¶œë ¥: ë‹¤ìŒì—” ë‚˜ë„ ë°ë¦¬ê³  ê°€ë¼ í•´ì•¼ì§€\n",
            "\n",
            "ìŒì„± íŒŒì¼: /content/drive/MyDrive/data_file/á„‘á…§á†¼á„€á…¡ á„ƒá…¦á„‹á…µá„á…¥á„‰á…¦á†º/K00013886-BFG23-L1N2D2-E-K0KK-02989139.wav\n",
            "í…ìŠ¤íŠ¸ ì¶œë ¥:  ëª¨ìë¥¼ ì“°ë‹ˆ ì¢€ ì‹œì›í•œ ëŠë‚Œì´ë‹¤.\n",
            "\n",
            "ìŒì„± íŒŒì¼: /content/drive/MyDrive/data_file/á„‘á…§á†¼á„€á…¡ á„ƒá…¦á„‹á…µá„á…¥á„‰á…¦á†º/K00014982-BFG20-L1N2D1-E-K0KK-03006747.wav\n",
            "í…ìŠ¤íŠ¸ ì¶œë ¥:  í•œìê³µë¶€ë¥¼ í•˜ë‹ˆ ëª¨ë¥´ëŠ” ë‹¨ì–´ë¥¼ ë§ì´ ì•Œê²Œ ë˜ì—ˆì–´ìš”.\n",
            "\n",
            "ìŒì„± íŒŒì¼: /content/drive/MyDrive/data_file/á„‘á…§á†¼á„€á…¡ á„ƒá…¦á„‹á…µá„á…¥á„‰á…¦á†º/K00014982-BFG20-L1N2D1-E-K0KK-03017978.wav\n",
            "í…ìŠ¤íŠ¸ ì¶œë ¥: ìì›ì„ ì¬í™œìš©í•˜ëŠ” ê±´ ì§€êµ¬ë¥¼ ìœ„í•´ì„œ ì¢‹ì€ ê±°ë˜ìš”\n",
            "\n",
            "ìŒì„± íŒŒì¼: /content/drive/MyDrive/data_file/á„‘á…§á†¼á„€á…¡ á„ƒá…¦á„‹á…µá„á…¥á„‰á…¦á†º/K00014982-BFG20-L1N2D4-E-K0KK-02872759.wav\n",
            "í…ìŠ¤íŠ¸ ì¶œë ¥:  ì±…ìƒ ìœ„ì— ì“°ë ˆê¸°ê°€ ë§ì•„ìš”.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import soundfile as sf  # soundfile ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©\n",
        "\n",
        "# ASR íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”\n",
        "model_name_or_path = \"/content/drive/MyDrive/model/whisper_base_0824\"\n",
        "asr = pipeline(model=model_name_or_path, task=\"automatic-speech-recognition\")\n",
        "\n",
        "# ìŒì„± íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸\n",
        "audio_file_paths = [\n",
        "    \"/content/drive/MyDrive/data_file/á„‘á…§á†¼á„€á…¡ á„ƒá…¦á„‹á…µá„á…¥á„‰á…¦á†º/K00013886-BFG23-L1N2D2-E-K0KK-02601769.wav\",\n",
        "    \"/content/drive/MyDrive/data_file/á„‘á…§á†¼á„€á…¡ á„ƒá…¦á„‹á…µá„á…¥á„‰á…¦á†º/K00013886-BFG23-L1N2D2-E-K0KK-02743434.wav\",\n",
        "    \"/content/drive/MyDrive/data_file/á„‘á…§á†¼á„€á…¡ á„ƒá…¦á„‹á…µá„á…¥á„‰á…¦á†º/K00013886-BFG23-L1N2D2-E-K0KK-02989139.wav\",\n",
        "    \"/content/drive/MyDrive/data_file/á„‘á…§á†¼á„€á…¡ á„ƒá…¦á„‹á…µá„á…¥á„‰á…¦á†º/K00014982-BFG20-L1N2D1-E-K0KK-03006747.wav\",\n",
        "    \"/content/drive/MyDrive/data_file/á„‘á…§á†¼á„€á…¡ á„ƒá…¦á„‹á…µá„á…¥á„‰á…¦á†º/K00014982-BFG20-L1N2D1-E-K0KK-03017978.wav\",\n",
        "    \"/content/drive/MyDrive/data_file/á„‘á…§á†¼á„€á…¡ á„ƒá…¦á„‹á…µá„á…¥á„‰á…¦á†º/K00014982-BFG20-L1N2D4-E-K0KK-02872759.wav\",\n",
        "    # ì¶”ê°€ ìŒì„± íŒŒì¼ ê²½ë¡œ\n",
        "]\n",
        "\n",
        "# ASR í•¨ìˆ˜ ì •ì˜\n",
        "def transcribe_audio(audio_path):\n",
        "    transcription = asr(audio_path)\n",
        "    return transcription['text']  # Use 'text' key to get the transcribed text\n",
        "\n",
        "# ê° ìŒì„± íŒŒì¼ì— ëŒ€í•œ ì²˜ë¦¬ ë° ì¶œë ¥\n",
        "for audio_file_path in audio_file_paths:\n",
        "    transcription_text = transcribe_audio(audio_file_path)\n",
        "    print(f\"ìŒì„± íŒŒì¼: {audio_file_path}\")\n",
        "    print(\"í…ìŠ¤íŠ¸ ì¶œë ¥:\", transcription_text)\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Njl9YkGxN7WX",
        "WTo-ygLdwTwp",
        "fYXaMpOtw6Lm",
        "SSLj6zSXKxbT"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
