{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 환경 설정"
      ],
      "metadata": {
        "id": "e90zKmPewaL2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umN-Qq9W3A1f"
      },
      "source": [
        "구글 드라이브와 Colab을 연동"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSqSE-DS297n",
        "outputId": "388aecb6-2e49-40ca-893d-840206620e81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnX1gx5vv_kc"
      },
      "source": [
        "\n",
        "Gpu 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KStnU69Dv-zz",
        "outputId": "1d92158e-3007-4acd-f5ef-ed9988a1a8b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnDQzwwLwC-c"
      },
      "source": [
        "whisper ai 사용에 필요한 python 패키지"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86z0bIqxeMki"
      },
      "outputs": [],
      "source": [
        "# 데이터셋과 transformers 설치\n",
        "!pip install datasets>=2.6.1\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "\n",
        "# 오디오 처리를 위한 librosa 설치\n",
        "!pip install librosa\n",
        "\n",
        "# 성능 측정을 위한 evaluate와 jiwer 설치\n",
        "!pip install evaluate>=0.30\n",
        "!pip install jiwer\n",
        "\n",
        "# 인터랙티브 인터페이스를 위한 gradio 설치\n",
        "!pip install gradio\n",
        "\n",
        "# Transformers와 PyTorch를 함께 사용하기 위한 accelerate 설치\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate>=0.20.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV65OMc_wGrJ"
      },
      "source": [
        "modules 과 packges import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcHXA33leT0A"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "import torch\n",
        "\n",
        "# import the relavant libraries for loggin in\n",
        "from huggingface_hub import HfApi, HfFolder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTo-ygLdwTwp"
      },
      "source": [
        "# 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcErvqAgefKS"
      },
      "outputs": [],
      "source": [
        "def login_hugging_face(token):\n",
        "    \"\"\"\n",
        "    Hugging Face API 토큰을 사용하여 Hugging Face에 로그인합니다.\n",
        "    \"\"\"\n",
        "    folder = HfFolder()\n",
        "    folder.save_token(token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_jIKSBAtTEo"
      },
      "source": [
        "모델에 입력으로 사용될 데이터와 라벨 데이터가 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "povAUjkcejQU"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(batch):\n",
        "    \"\"\"\n",
        "    Whisper AI 모델에 적합한 형식으로 오디오 데이터를 준비합니다.\n",
        "    \"\"\"\n",
        "    # 오디오 데이터를 48kHz에서 16kHz로 변환하여 로드\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # 입력 오디오 배열로부터 log-Mel 입력 특성을 계산\n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # 대상 텍스트를 토큰화하고 라벨 ID로 인코딩\n",
        "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHs-HkuJtbXe"
      },
      "source": [
        "데이터 콜레이터 클래스는 ASR 모델의 훈련 및 평가 과정에서 데이터의 전처리와 패딩을 처리하는 역할을 수행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQEmML4Zex5z"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    \"\"\"\n",
        "    Use Data Collator to perform Speech Seq2Seq with padding\n",
        "    \"\"\"\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_qwfaqUvf4w"
      },
      "source": [
        "평기지표: CER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg9QFaBGe2nj"
      },
      "outputs": [],
      "source": [
        "from jiwer import wer\n",
        "\n",
        "def compute_cer(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    cer = wer(label_str, pred_str)\n",
        "\n",
        "    return {\"cer\": cer}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT9LBBXdwIsi"
      },
      "source": [
        "# STEP 0. Hugging Face 로그인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywDnKqWTe5Hu",
        "outputId": "62e87eea-e04e-438f-99ec-f47d9da25677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We are logged in to Hugging Face now!\n"
          ]
        }
      ],
      "source": [
        "# get your account token from https://huggingface.co/settings/tokens\n",
        "token = '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'\n",
        "login_hugging_face(token)\n",
        "print('We are logged in to Hugging Face now!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qatnD5H2waD8"
      },
      "source": [
        "# STEP 1. 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rxOZH0NiZ3q",
        "outputId": "cc9b733d-80a8-4966-cfb5-3abf24c372de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio Files:\n",
            "K00017752-BMG20-L1N2D1-E-K0KK-04134270.wav\n",
            "K00017750-BFG32-L1N2D1-E-K0KK-04585267.wav\n",
            "K00017751-BMG23-L1N2D1-E-K0KK-04365748.wav\n",
            "\n",
            "Label Files:\n",
            "K00017744-BMG30-L1N2D3-E-K0KK-04399437.txt\n",
            "K00017867-BFG23-L1N2D2-E-K0KK-04650352.txt\n",
            "K00017759-BMG30-L1N2D4-E-K0KK-04423102.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "def extract_zip_files(zip_file_path, extracted_folder_path):\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "def change_folder_structure(extracted_folder_path):\n",
        "    audio_target_dir = os.path.join(extracted_folder_path, \"audio\")\n",
        "    label_target_dir = os.path.join(extracted_folder_path, \"labels\")\n",
        "\n",
        "    # 폴더 생성\n",
        "    os.makedirs(audio_target_dir, exist_ok=True)\n",
        "    os.makedirs(label_target_dir, exist_ok=True)\n",
        "\n",
        "    for root, _, files in os.walk(extracted_folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".wav\"):\n",
        "                audio_file_path = os.path.join(root, file)\n",
        "                new_audio_file_path = os.path.join(audio_target_dir, file)\n",
        "                shutil.move(audio_file_path, new_audio_file_path)\n",
        "\n",
        "            elif file.endswith(\".json\"):\n",
        "                label_file_path = os.path.join(root, file)\n",
        "                audio_id = file.split(\".\")[0]\n",
        "                new_label_file_path = os.path.join(label_target_dir, f\"{audio_id}.txt\")\n",
        "                shutil.move(label_file_path, new_label_file_path)\n",
        "\n",
        "    # 빈 폴더 제거 (오직 audio_target_dir와 label_target_dir만 남아있을 것)\n",
        "    for root, dirs, _ in os.walk(extracted_folder_path, topdown=False):\n",
        "        for dir in dirs:\n",
        "            if dir not in [os.path.basename(audio_target_dir), os.path.basename(label_target_dir)]:\n",
        "                dir_path = os.path.join(root, dir)\n",
        "                os.rmdir(dir_path)\n",
        "\n",
        "# 2. 데이터 파일이 저장된 경로를 지정합니다.\n",
        "zip_file_path_audio = '/content/drive/MyDrive/data_file/sample_15000_ko_child/audio_sample.zip'\n",
        "zip_file_path_label = '/content/drive/MyDrive/data_file/sample_15000_ko_child/label_sample.zip'\n",
        "extracted_folder_path = '/content/data/'  # 압축 해제된 데이터가 저장될 폴더 경로\n",
        "\n",
        "# 폴더 초기화\n",
        "if os.path.exists(extracted_folder_path):\n",
        "    shutil.rmtree(extracted_folder_path)\n",
        "\n",
        "# 3. zip 파일을 압축 해제합니다.\n",
        "extract_zip_files(zip_file_path_audio, extracted_folder_path)\n",
        "extract_zip_files(zip_file_path_label, extracted_folder_path)\n",
        "\n",
        "# 4. 폴더 구조 변경\n",
        "change_folder_structure(extracted_folder_path)\n",
        "\n",
        "# 5. 구조 확인\n",
        "# 데이터 폴더 경로 설정\n",
        "data_folder = '/content/data/'\n",
        "\n",
        "# audio 폴더 내 파일 목록 확인\n",
        "audio_files = os.listdir(os.path.join(data_folder, 'audio'))\n",
        "print(\"Audio Files:\")\n",
        "for i in range(3):  # 최대 3개 파일만 출력\n",
        "    if i < len(audio_files):\n",
        "        print(audio_files[i])\n",
        "\n",
        "# labels 폴더 내 파일 목록 확인\n",
        "label_files = os.listdir(os.path.join(data_folder, 'labels'))\n",
        "print(\"\\nLabel Files:\")\n",
        "for i in range(3):  # 최대 3개 파일만 출력\n",
        "    if i < len(label_files):\n",
        "        print(label_files[i])\n",
        "\n",
        "# 이제 \"/content/data/\" 폴더 내에 \"audio\" 폴더와 \"labels\" 폴더가 생성되며,\n",
        "# 해당 폴더에 원천데이터인 wav 파일과 라벨링데이터인 txt 파일이 저장됩니다.\n",
        "# 이후에는 해당 경로에서 데이터를 불러와서 학습을 진행하시면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7WR1Atowdo5"
      },
      "source": [
        "audio data 개수와 label data 개수 같은지 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jVpPltDlYAL",
        "outputId": "9693a5fd-3360-4686-be61-7233b2255af9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "오디오 파일 개수: 15953\n",
            "JSON 파일 개수: 15953\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "data_folder = \"/content/data\"\n",
        "audio_folder = os.path.join(data_folder, \"audio\")\n",
        "label_folder = os.path.join(data_folder, \"labels\")\n",
        "\n",
        "# audio 폴더 내의 파일 개수 확인\n",
        "audio_files = [file for file in os.listdir(audio_folder) if file.endswith(\".wav\")]\n",
        "print(f\"오디오 파일 개수: {len(audio_files)}\")\n",
        "\n",
        "# labels 폴더 내의 파일 개수 확인\n",
        "label_files = [file for file in os.listdir(label_folder) if file.endswith(\".txt\")]\n",
        "print(f\"JSON 파일 개수: {len(label_files)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNbgGyYjwm6P"
      },
      "source": [
        "train과 test 데이터로 나누기 및 모델 학습을 위한 구조 변경"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNqAcayci4BN",
        "outputId": "f8edd5ca-87cb-4cb4-e06d-43c2553d8577"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['audio', 'sentence'],\n",
            "        num_rows: 14357\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['audio', 'sentence'],\n",
            "        num_rows: 1596\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import DatasetDict, Dataset, load_dataset\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "\n",
        "def create_dataset(data_folder):\n",
        "    audio_folder = os.path.join(data_folder, \"audio\")\n",
        "    label_folder = os.path.join(data_folder, \"labels\")\n",
        "\n",
        "    # audio 폴더 내의 WAV 파일 리스트를 가져옴\n",
        "    audio_files = [file for file in os.listdir(audio_folder) if file.endswith(\".wav\")]\n",
        "\n",
        "    # label 폴더 내의 JSON 파일 리스트를 가져옴\n",
        "    label_files = [file for file in os.listdir(label_folder) if file.endswith(\".txt\")]\n",
        "\n",
        "    # WAV 파일과 JSON 파일이 정상적으로 매칭되어야 함\n",
        "    assert len(audio_files) == len(label_files)\n",
        "\n",
        "    # 데이터셋 생성\n",
        "    dataset_dict = DatasetDict()\n",
        "    train_data = {\"audio\": [], \"sentence\": []}\n",
        "    test_data = {\"audio\": [], \"sentence\": []}\n",
        "\n",
        "    # 데이터를 무작위로 섞어서 train과 test로 나누기\n",
        "    data_pairs = list(zip(audio_files, label_files))\n",
        "    random.shuffle(data_pairs)\n",
        "\n",
        "    train_size = int(len(data_pairs) * 0.9)\n",
        "\n",
        "    for audio_file, label_file in data_pairs[:train_size]:\n",
        "        # WAV 파일과 JSON 파일을 읽어서 train 데이터셋에 추가\n",
        "        audio_path = os.path.join(audio_folder, audio_file)\n",
        "        label_path = os.path.join(label_folder, label_file)\n",
        "\n",
        "        with open(label_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            label_data = json.load(f)\n",
        "            sentence = label_data[\"Transcription\"][\"LabelText\"]\n",
        "\n",
        "        train_data[\"audio\"].append(audio_path)\n",
        "        train_data[\"sentence\"].append(sentence)\n",
        "\n",
        "    for audio_file, label_file in data_pairs[train_size:]:\n",
        "        # WAV 파일과 JSON 파일을 읽어서 test 데이터셋에 추가\n",
        "        audio_path = os.path.join(audio_folder, audio_file)\n",
        "        label_path = os.path.join(label_folder, label_file)\n",
        "\n",
        "        with open(label_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            label_data = json.load(f)\n",
        "            sentence = label_data[\"Transcription\"][\"LabelText\"]\n",
        "\n",
        "        test_data[\"audio\"].append(audio_path)\n",
        "        test_data[\"sentence\"].append(sentence)\n",
        "\n",
        "    # 데이터셋에 추가\n",
        "    dataset_dict[\"train\"] = Dataset.from_dict(train_data)\n",
        "    dataset_dict[\"test\"] = Dataset.from_dict(test_data)\n",
        "\n",
        "    return dataset_dict\n",
        "\n",
        "# 데이터 폴더 경로 지정\n",
        "data_folder = \"/content/data\"\n",
        "\n",
        "# 데이터셋 생성\n",
        "custom_dataset = create_dataset(data_folder)\n",
        "\n",
        "# \"sentence\" 필드만 출력\n",
        "print(custom_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bphs7qjGwqov"
      },
      "source": [
        "audio data 와 text data 일부 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5YEK2-Yl2jr",
        "outputId": "acfd6df4-758a-40e9-f554-94b7ed27cc1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "오디오 데이터 일부 출력:\n",
            "/content/data/audio/K00017750-BFG32-L1N2D1-E-K0KK-04628309.wav\n",
            "\n",
            "텍스트 문장 데이터 일부 출력:\n",
            "풀이 움직이고 나무가 흔들렸다\n"
          ]
        }
      ],
      "source": [
        "# 오디오 데이터 일부 출력\n",
        "print(\"오디오 데이터 일부 출력:\")\n",
        "print(custom_dataset[\"train\"][\"audio\"][2])\n",
        "print()\n",
        "\n",
        "# 텍스트 문장 데이터 일부 출력\n",
        "print(\"텍스트 문장 데이터 일부 출력:\")\n",
        "print(custom_dataset[\"train\"][\"sentence\"][2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYXaMpOtw6Lm"
      },
      "source": [
        "# STEP 2. Feature Extractor, Tokenizer and Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ljILt1DpwcJ"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "from transformers import WhisperTokenizer\n",
        "\n",
        "# - Load Feature extractor: WhisperFeatureExtractor\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
        "\n",
        "# - Load Tokenizer: WhisperTokenizer\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"korean\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRlUZ0PmxAEL"
      },
      "source": [
        "# STEP 3. Combine elements with WhisperProcessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clpQzKlqpzq3"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperProcessor\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"korean\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzx2yMdDxVJv"
      },
      "source": [
        "# STEP 4. 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "5fc44717e6ed434f9cc322ca6a974712",
            "65a72b3de74647029551c02fef1e2dd0",
            "c9e3420016654349b7aba4017d7ecbd9",
            "c34deb57736e48fbab6891c01e58b7a4",
            "756d9b65fa85442da269cd22165a7150",
            "3994abcd7a264c7d8e16f6c2e0a91896",
            "b919cc294caf457ea665e1496fe7496e",
            "51b322faedd94e9c85b3f10cf98950ec",
            "fcf54f0decb64f45a192724fc121aaa6",
            "823db121f6f54e84848b9513723eb8eb",
            "d10bb146030a40598122d8b01ce8b6e3",
            "e29baae86de348cc8b7e11eb86d8e5f2",
            "e25b344c5f5d477aaafa9c1d1aac8215",
            "6d78a0d3ecf64afaa1b8d75b0d028c53",
            "c4685eb3fa62490086160ec8f7b9376d",
            "cfef1928639e41c3b3063e371cf72d8a",
            "06f4fb8359334517ad706af5ceb37e1f",
            "c3e026619eb141af9dcf5cd407514ba5",
            "46d558ebe2044f68a71b08766e0aba8e",
            "09072746360e4a38884ed67358e3ddde",
            "282c928beee243c4b94b4073713ee524",
            "5d25cfe8e9a34303bb081a337832fd5b"
          ]
        },
        "id": "luZjHLr2p2PU",
        "outputId": "008c8da8-8a7c-4c41-dc9d-68ac973541ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Check the random audio example from custom_dataset dataset to see what form the data is in:\n",
            "{'audio': '/content/data/audio/K00017867-BFG23-L1N2D2-E-K0KK-04553302.wav', 'sentence': '수업 시간에 시를 읽어보기도 했다.'}\n",
            "\n",
            "| Check the effect of downsampling:\n",
            "{'audio': {'path': '/content/data/audio/K00017867-BFG23-L1N2D2-E-K0KK-04553302.wav', 'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "       3.35693359e-04, 1.52587891e-04, 3.05175781e-05]), 'sampling_rate': 16000}, 'sentence': '수업 시간에 시를 읽어보기도 했다.'}\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/14357 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fc44717e6ed434f9cc322ca6a974712"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1596 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e29baae86de348cc8b7e11eb86d8e5f2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "print('| Check the audio example')\n",
        "print(f'{custom_dataset[\"train\"][0]}\\n')\n",
        "\n",
        "# Downsample from 48kHZ to 16kHZ\n",
        "# Whisper AI 모델이 16kHz의 샘플링 속도에서 훈련되어서\n",
        "# 입력 데이터를 16kHz로 다운샘플링하는 것이 필요\n",
        "from datasets import Audio\n",
        "custom_dataset = custom_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "\n",
        "print('| Check the effect of downsampling:')\n",
        "print(f'{custom_dataset[\"train\"][0]}\\n')\n",
        "\n",
        "# Prepare and use function to prepare our data ready for the Whisper AI model\n",
        "custom_dataset = custom_dataset.map(\n",
        "    prepare_dataset,\n",
        "    remove_columns=custom_dataset.column_names[\"train\"],\n",
        "    num_proc=1 # num_proc > 1 will enable multiprocessing / num_proc = 2 -> 1로 변경함\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRFES7w9xava"
      },
      "source": [
        "# STEP 5. Training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceB98ofDqR2u"
      },
      "outputs": [],
      "source": [
        "# STEP 5.1. Initialize the Data collator\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
        "\n",
        "# STEP 5.1. Define evaluation metric\n",
        "import evaluate\n",
        "metric = evaluate.load(\"cer\")\n",
        "\n",
        "# STEP 5.3. Load a pre-trained Checkpoint\n",
        "from transformers import WhisperForConditionalGeneration\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
        "\n",
        "\"\"\"\n",
        "Overide generation arguments:\n",
        "- no tokens are forced as decoder outputs: https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.forced_decoder_ids\n",
        "- no tokens are suppressed during generation: https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.suppress_tokens\n",
        "\"\"\"\n",
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gGTa9HVqXov",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "7f08b773-60b2-4455-b195-b4c9335ef9e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training is started.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 3:43:21, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Cer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>4.884100</td>\n",
              "      <td>3.509566</td>\n",
              "      <td>1.684131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.148400</td>\n",
              "      <td>2.921551</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.827700</td>\n",
              "      <td>2.823291</td>\n",
              "      <td>1.123006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.806400</td>\n",
              "      <td>2.778835</td>\n",
              "      <td>1.778245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.746400</td>\n",
              "      <td>2.755793</td>\n",
              "      <td>2.965502</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training is finished.\n"
          ]
        }
      ],
      "source": [
        "# STEP 5.4. Define the training configuration\n",
        "\"\"\"\n",
        "Check for Seq2SeqTrainingArguments here:\n",
        "https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments\n",
        "\"\"\"\n",
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-small-ver1\",  # 저장된 모델 및 결과물의 디렉토리 경로\n",
        "    per_device_train_batch_size=16,  # 한 번에 처리되는 훈련 배치 크기\n",
        "    gradient_accumulation_steps=1,  # 배치 크기 감소시 그래디언트 누적을 통한 학습 안정화\n",
        "    learning_rate=1e-5,  # 학습률\n",
        "    warmup_steps=200,  # 초기 학습률 조정을 위한 웜업 스텝 수 / 일반적으로는 10% ~ 20%의 전체 학습 스텝 수에 해당하는 값을 시도\n",
        "    max_steps=500,  # 전체 훈련 스텝 수\n",
        "    gradient_checkpointing=True,  # 그래디언트 체크포인팅을 통한 메모리 절약\n",
        "    # fp16=True,  # FP16 형식으로 훈련 수행 (반정밀도 부동소수점)( cpu 가동시 안씀)\n",
        "    evaluation_strategy=\"steps\",  # 검증 수행 전략 설정\n",
        "    per_device_eval_batch_size=8,  # 한 번에 처리되는 검증 배치 크기\n",
        "    predict_with_generate=True,  # 생성된 토큰을 통해 예측 수행\n",
        "    generation_max_length=225,  # 생성된 토큰의 최대 길이 (225 -> 800으로 늘림)\n",
        "    eval_steps=100,  # 검증 수행 스텝 수\n",
        "    logging_steps=100,  # 로그 기록 스텝 수\n",
        "    load_best_model_at_end=False,  # 훈련 종료 시 최적 모델 로드 여부\n",
        "    metric_for_best_model=\"cer\",  # 최적 모델 선정을 위한 평가 지표 wer -> cer로 변경\n",
        "    greater_is_better=False,  # 평가 지표 값이 높을수록 좋은지 여부\n",
        "    save_steps=100  # 변경된 save_steps 값\n",
        ")\n",
        "\n",
        "# Initialize a trainer.\n",
        "\"\"\"\n",
        "Hugging Face trainer에 다음의 값을 전달\n",
        "training arguments, model, dataset, data collator and compute_metrics function.\n",
        "\"\"\"\n",
        "# 지정된 인자 및 구성요소로 Initialize a trainer.\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,                   # 이전에 정의한 훈련 인자\n",
        "    model=model,                          # 훈련할 ASR 모델\n",
        "    train_dataset=custom_dataset[\"train\"],# 훈련 데이터셋\n",
        "    eval_dataset=custom_dataset[\"test\"],  # 평가 데이터셋\n",
        "    data_collator=data_collator,           # 데이터 전처리를 위한 데이터 콜레이터\n",
        "    compute_metrics=compute_cer,          # CER 메트릭을 계산하는 함수\n",
        "    tokenizer=processor.feature_extractor, # 입력 오디오 데이터를 처리하기 위한 토크나이저\n",
        ")\n",
        "\n",
        "# training 시작전에 processor object 저장\n",
        "processor.save_pretrained(training_args.output_dir)\n",
        "\n",
        "# STEP 5.5. Training\n",
        "\"\"\"\n",
        "Training will take appr. 5-10 hours depending on your GPU.\n",
        "\"\"\"\n",
        "print('Training 시작')\n",
        "trainer.train()  # <-- training 시작\n",
        "print('Training 완료')\n",
        "\n",
        "#\"Step\": 모델의 훈련 과정에서 진행되는 각 스텝을 나타내는 숫자입니다.\n",
        "#스텝은 주로 배치(batch) 단위로 모델이 업데이트되는 지점을 의미합니다.\n",
        "\n",
        "# \"Training Loss\": 훈련 데이터를 사용하여 모델을 학습할 때 나타나는 손실 값입니다.\n",
        "#이 값은 모델이 예측한 결과와 실제 정답과의 차이를 나타냅니다.\n",
        "\n",
        "# \"Validation Loss\": 훈련 중에 일정 주기마다 검증 데이터를 사용하여 모델의 성능을\n",
        "# 평가한 후 나타나는 손실 값입니다.\n",
        "# 이 값도 마찬가지로 모델의 예측 결과와 실제 정답과의 차이를 나타냅니다.\n",
        "# 검증 손실이 감소하는 것은 모델이 일반화되는 표시입니다.\n",
        "\n",
        "# \"CER\" (Character Error Rate): 훈련 중에 일정 주기마다 검증 데이터를 사용하여\n",
        "#모델의 문자 에러 비율(CER)을 평가한 값입니다.\n",
        "#CER은 텍스트 분야에서 자주 사용되는 평가 지표 중 하나로,\n",
        "#모델이 생성한 텍스트와 실제 텍스트 사이의 문자 수준 오류 비율을 나타냅니다.\n",
        "#CER이 낮을수록 모델의 성능이 좋다고 판단됩니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic1yCT8HuUI8"
      },
      "source": [
        "학습된 모델 평가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPMYJbdee4QU"
      },
      "outputs": [],
      "source": [
        "# trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGxBKiX1EQVC"
      },
      "source": [
        "학습된 모델 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liHO1-IIEP4O"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/model/whisper_small_0811_ver1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg2zOyXUx4ND"
      },
      "source": [
        "모델에 토크나이저 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wiaehlroqf-r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d90e19a-455c-4936-b5b7-11d04be02658"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/model/whisper_small_0811_ver1/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/model/whisper_small_0811_ver1/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/model/whisper_small_0811_ver1/vocab.json',\n",
              " '/content/drive/MyDrive/model/whisper_small_0811_ver1/merges.txt',\n",
              " '/content/drive/MyDrive/model/whisper_small_0811_ver1/normalizer.json',\n",
              " '/content/drive/MyDrive/model/whisper_small_0811_ver1/added_tokens.json',\n",
              " '/content/drive/MyDrive/model/whisper_small_0811_ver1/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the trained tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/model/whisper_small\")\n",
        "\n",
        "# Specify the directory where you want to save the tokenizer\n",
        "save_directory = \"/content/drive/MyDrive/model/whisper_small_0811_ver1\"\n",
        "\n",
        "# Save the tokenizer to the specified directory\n",
        "tokenizer.save_pretrained(save_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4Ucc7_ZuaHa"
      },
      "source": [
        "모델 파일내용 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTNvGXJ4uamH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7e2d001-2082-44f9-c03f-852a5f8ed95c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['config.json', 'generation_config.json', 'pytorch_model.bin', 'preprocessor_config.json', 'training_args.bin', 'tokenizer_config.json', 'special_tokens_map.json', 'added_tokens.json', 'vocab.json', 'merges.txt', 'normalizer.json', 'tokenizer.json']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "model_path = \"/content/drive/MyDrive/model/whisper_small_0811_ver1\"\n",
        "files_in_model_path = os.listdir(model_path)\n",
        "print(files_in_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 6. Model use"
      ],
      "metadata": {
        "id": "SAvxlHpuDdGP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtMD0NWiucha"
      },
      "source": [
        "모델 실행코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GtKF4UOueAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "049345dd-5ee2-4616-9d87-bbebbf20de9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=448) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription for /content/drive/MyDrive/data_file/whisper 테스트 음원/audio_21_11_17_3596/K00013596-BMG23-L1N2D1-E-K0KK-02166463.wav\n",
            " మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు �\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=448) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription for /content/drive/MyDrive/data_file/whisper 테스트 음원/audio_21_11_17_3596/K00013596-BMG23-L1N2D1-E-K0KK-02164794.wav\n",
            " మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు �\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=448) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription for /content/drive/MyDrive/data_file/whisper 테스트 음원/audio_21_11_17_3596/K00013596-BMG23-L1N2D1-E-K0KK-02165323.wav\n",
            " మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు మార్లు �\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_name_or_path = \"/content/drive/MyDrive/model/whisper_small_0811_ver1\"\n",
        "asr = pipeline(model=model_name_or_path, task=\"automatic-speech-recognition\")\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    transcription = asr(audio_path)\n",
        "    return transcription['text']  # Use 'text' key to get the transcribed text\n",
        "\n",
        "audio_file_paths = [\n",
        "    \"/content/drive/MyDrive/data_file/whisper 테스트 음원/audio_21_11_17_3596/K00013596-BMG23-L1N2D1-E-K0KK-02166463.wav\",\n",
        "    \"/content/drive/MyDrive/data_file/whisper 테스트 음원/audio_21_11_17_3596/K00013596-BMG23-L1N2D1-E-K0KK-02164794.wav\",\n",
        "    \"/content/drive/MyDrive/data_file/whisper 테스트 음원/audio_21_11_17_3596/K00013596-BMG23-L1N2D1-E-K0KK-02165323.wav\"\n",
        "    # 음성 파일 추가가능\n",
        "]\n",
        "\n",
        "for audio_file_path in audio_file_paths:\n",
        "    transcription_text = transcribe_audio(audio_file_path)\n",
        "    print(\"Transcription for\", audio_file_path)\n",
        "    print(transcription_text)\n",
        "    print(\"=\" * 50)  # Separate each transcription with a line of '=' for clarity"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "fYXaMpOtw6Lm"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5fc44717e6ed434f9cc322ca6a974712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_65a72b3de74647029551c02fef1e2dd0",
              "IPY_MODEL_c9e3420016654349b7aba4017d7ecbd9",
              "IPY_MODEL_c34deb57736e48fbab6891c01e58b7a4"
            ],
            "layout": "IPY_MODEL_756d9b65fa85442da269cd22165a7150"
          }
        },
        "65a72b3de74647029551c02fef1e2dd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3994abcd7a264c7d8e16f6c2e0a91896",
            "placeholder": "​",
            "style": "IPY_MODEL_b919cc294caf457ea665e1496fe7496e",
            "value": "Map: 100%"
          }
        },
        "c9e3420016654349b7aba4017d7ecbd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51b322faedd94e9c85b3f10cf98950ec",
            "max": 14357,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fcf54f0decb64f45a192724fc121aaa6",
            "value": 14357
          }
        },
        "c34deb57736e48fbab6891c01e58b7a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_823db121f6f54e84848b9513723eb8eb",
            "placeholder": "​",
            "style": "IPY_MODEL_d10bb146030a40598122d8b01ce8b6e3",
            "value": " 14357/14357 [18:08&lt;00:00, 15.79 examples/s]"
          }
        },
        "756d9b65fa85442da269cd22165a7150": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3994abcd7a264c7d8e16f6c2e0a91896": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b919cc294caf457ea665e1496fe7496e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51b322faedd94e9c85b3f10cf98950ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcf54f0decb64f45a192724fc121aaa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "823db121f6f54e84848b9513723eb8eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d10bb146030a40598122d8b01ce8b6e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e29baae86de348cc8b7e11eb86d8e5f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e25b344c5f5d477aaafa9c1d1aac8215",
              "IPY_MODEL_6d78a0d3ecf64afaa1b8d75b0d028c53",
              "IPY_MODEL_c4685eb3fa62490086160ec8f7b9376d"
            ],
            "layout": "IPY_MODEL_cfef1928639e41c3b3063e371cf72d8a"
          }
        },
        "e25b344c5f5d477aaafa9c1d1aac8215": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06f4fb8359334517ad706af5ceb37e1f",
            "placeholder": "​",
            "style": "IPY_MODEL_c3e026619eb141af9dcf5cd407514ba5",
            "value": "Map: 100%"
          }
        },
        "6d78a0d3ecf64afaa1b8d75b0d028c53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46d558ebe2044f68a71b08766e0aba8e",
            "max": 1596,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09072746360e4a38884ed67358e3ddde",
            "value": 1596
          }
        },
        "c4685eb3fa62490086160ec8f7b9376d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_282c928beee243c4b94b4073713ee524",
            "placeholder": "​",
            "style": "IPY_MODEL_5d25cfe8e9a34303bb081a337832fd5b",
            "value": " 1596/1596 [01:56&lt;00:00, 14.26 examples/s]"
          }
        },
        "cfef1928639e41c3b3063e371cf72d8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06f4fb8359334517ad706af5ceb37e1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3e026619eb141af9dcf5cd407514ba5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46d558ebe2044f68a71b08766e0aba8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09072746360e4a38884ed67358e3ddde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "282c928beee243c4b94b4073713ee524": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d25cfe8e9a34303bb081a337832fd5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}